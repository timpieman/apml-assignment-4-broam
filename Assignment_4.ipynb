{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 – Finding the way out in a maze\n",
    "\n",
    "*Due: Friday January 27 at 17:00 CET*\n",
    "\n",
    "In the forth assignment of the course Applications of Machine Learning (INFOB3APML), you will learn to use RL algorithms to solve the practical problem of ‘robot in a maze’. The objectives of this assignment are:\n",
    "- learn to formalize a practical problem into the Markov Decision Process (MDP)\n",
    "- get familier with the OpenAI gym framework (recently renamed as Gymnasium) and use it to implement RL agents\n",
    "- use the SARSA and Q-learning algorithm to solve the ‘robot in a maze’ MDP problem\n",
    "- evaluate the results of reinforcement learning and interpret your findings\n",
    "- reflect on the difference between two type of RL algorithms\n",
    "\n",
    "In this assignment, you are going to develop a robot and find its way out in a maze. The project is divided into three parts (5 subtask):\n",
    "-\tIn the first part (1), you will get familier with the OpenAI gym/gymnasium framework. \n",
    "-\tIn the second part, based on the gym/gymnasium framework, we have implemented the environment for you (2.1). Your task is to formalize the problem as a MDP model (2.0), implement your own RL agents (2.2) and to train them for finding the shortest route to get out of a maze (2.3).\n",
    "-\tIn the third part (3), you will evaluate and interpret your results from the implemented RL agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's start with the OpenAI gym\n",
    "\n",
    "Gym/Gynasium (https://gymnasium.farama.org/) is a wide-used standard toolkit for developing and comparing reinforcement learning algorithms. Gynasium is the maintained fork of OpenAI’s Gym library (more story about this recent change if you are interested: https://farama.org/Announcing-The-Farama-Foundation).\n",
    "\n",
    "1. Gym/Gynasium makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. \n",
    "\n",
    "2. The library is a collection of test problems — **environments** — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.\n",
    "\n",
    "First, we download & install the gym/gynasium library. Then import the gymnasium class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.27.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (0.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (4.12.0)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (1.23.3)\n",
      "Requirement already satisfied: shimmy<1.0,>=0.1.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (0.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.8.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to explain how the RL framework of gym works. \n",
    "- An **ENVIRONMENT**, \n",
    "- You also have an **AGENT**,\n",
    "- In MDP problems (like ours), the **ENVIRONMENT** will also provides an **OBSERVATION**, which represets the state of the **ENVIRONMENT** at the current moment.\n",
    "- The agent takes an **ACTION** based on its **OBSERVATION**,\n",
    "- When a single **ACTION** is chosen and fed to our **ENVIRONMENT**, the **ENVIRONMENT** measures how good the action was taken and produces a **REWARD**, which is usually a numeric value.\n",
    "\n",
    "Please read the 'Basic usage' https://gymnasium.farama.org/content/basic_usage/ for better understanding the framework.  And do not forget import gymnasium before running other codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Go back to our own task\n",
    " \n",
    " Next, you will solve a practical MDP problem 'robot in a maze' based on the gym framework. You shall implement the RL agent and train it to find the shortest route to achieve the maze goal. In this MDP, the enviroment is a grid world (a maze) while the agent is a robot. At each time step, the robot starts at a random location and can move around in the grid world. The long-term objective is finding the way out (reaching the final location). Hence, you need to find a fixed goal position within the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Model the practical task into a MDP\n",
    "\n",
    "To solve a RL problem, we start with formalizing the problem into a MDP model. Please describe this MDP model in your report. \n",
    "\n",
    "Notice: No empricial data provided in this assignment, so the point of 'data description and exploration' will be given to this step. \n",
    "\n",
    "While exploring your MDP model, you shall think about questions such as:\n",
    "- What is the environment? How does it look like?\n",
    "- What simulated data can your RL agent observe from the environment? How does it look like?\n",
    "- Which data is considered as the state? Which data is considered as the reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set up the environment\n",
    "\n",
    "There is no need to implement your own environment. You shall use the environment we provide in the file **environment.py**. But please make sure to have a look at it, so that you understand the inner working of this environment.\n",
    "\n",
    "The core gym interface is **Env**, which is the unified environment interface. The following are the Env methods you should know:\n",
    "\n",
    "- reset(self): Reset the environment's state. Returns observation.\n",
    "- step(self, action): Step the environment by one timestep. Returns observation, reward, done, info. \n",
    "- render(self, mode='rgb_array'): Render one frame of the environment. The default mode will do something human friendly, such as pop up a window. In this assignment, there is no need to create a pop up window. \n",
    "\n",
    "Please notice that you need to first install the [mazelab](https://github.com/zuoxingdong/mazelab) package for running the environment (a file with required packages is also given). If you run the below cell the first time. Make sure to restart the ipython notebook at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mazelab' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Jeroen/Documents/GitHub/apml-assignment-4-broam/mazelab\n",
      "Requirement already satisfied: gym in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mazelab==0.2.0) (0.26.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mazelab==0.2.0) (1.23.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mazelab==0.2.0) (3.6.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mazelab==0.2.0) (0.19.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym->mazelab==0.2.0) (4.12.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym->mazelab==0.2.0) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym->mazelab==0.2.0) (2.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.8.0->gym->mazelab==0.2.0) (3.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->mazelab==0.2.0) (4.37.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jeroen\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->mazelab==0.2.0) (1.15.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image->mazelab==0.2.0) (2.24.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image->mazelab==0.2.0) (3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image->mazelab==0.2.0) (2022.10.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image->mazelab==0.2.0) (1.4.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image->mazelab==0.2.0) (1.9.1)\n",
      "Installing collected packages: mazelab\n",
      "  Attempting uninstall: mazelab\n",
      "    Found existing installation: mazelab 0.2.0\n",
      "    Uninstalling mazelab-0.2.0:\n",
      "      Successfully uninstalled mazelab-0.2.0\n",
      "  Running setup.py develop for mazelab\n",
      "Successfully installed mazelab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.23.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jeroen\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.23.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.37.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jeroen\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jeroen\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sw1989/mazelab.git\n",
    "!pip install -e mazelab\n",
    "!pip install pandas\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check whether the required packages (e.g. mazela, pandas, tqdm, seaborn) are installed. Please install the ones are missing. \n",
    "\n",
    "ATTENTION: To run the given code, please use the python version 3.7-3.9, and the numpy version < 1.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a few helper functions to make it easier to debug your agents. \n",
    " - `animate_run` will enable you to see the agent's behavior. It takes a list of images which can be produced by the `env.render` function of the environment\n",
    " - `visualize_agent_brain` will provide you with a way to visualize the agents learned q_table. Use it after you have implemented and trained your agents. The first plot will show the highest q-value per state (position on the map) and the second will tell you which action the agent would choose at that state/position. It takes the environment and the agent as input.\n",
    "\n",
    "Below you will find a basic example of how the animation function works. Please notice that: whenever you **reset()** the environment, the agent will start at a random position (a different state). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa7UlEQVR4nO3df2zUhf3H8dfRk6Mj5bR1tL3ZQmeIyA8RRYhitiM2kgZRsq8yDGKDic6t/Kg1DLqtuKlw4jZWQVLEZMISfvmHICMR0nUUJONna51kGz9ih6ekdCbakyIn6X2+f3y/3PdbaYGjn+N9dzwfyeeP+9yn93l/bI9nPtePn3ocx3EEAMA11s96AADA9YkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE17rAb4tFovp1KlTysnJkcfjsR4HAJAgx3H01VdfKRAIqF+/3s9zUi5Ap06dUlFRkfUYAIA+CofDuuWWW3p9PuUClJOTI0lavny5srOzjacBACTq66+/VlVVVfzf896kXIAufOyWnZ1NgAAgjV3u1yhchAAAMEGAAAAmCBAAwAQBAgCYIEAAABNJC9CqVas0dOhQDRgwQBMmTNDBgweTtSsAQBpKSoA2b96sqqoqvfDCC2pubtaYMWM0efJktbe3J2N3AIA0lJQALV++XE8//bRmz56tESNGaPXq1frOd76jP/7xj8nYHQAgDbkeoG+++UZNTU0qLS39v53066fS0lLt27fvou2j0agikUi3BQCQ+VwP0Oeff66uri7l5+d3W5+fn6+2traLtg+FQvL7/fGF+8ABwPXB/Cq46upqdXR0xJdwOGw9EgDgGnD9XnA333yzsrKydPr06W7rT58+rYKCgou29/l88vl8bo8BAEhxrp8B9e/fX3fffbcaGhri62KxmBoaGnTvvfe6vTsAQJpKyt2wq6qqVF5ernHjxmn8+PGqra1VZ2enZs+enYzdAQDSUFIC9OMf/1j/+c9/tHjxYrW1tenOO+/Ujh07LrowAQBw/Ura3wOaM2eO5syZk6yXBwCkOfOr4AAA1ycCBAAwQYAAACYIEADABAECAJhI2lVwqS4YDFqPAAAmGhsbrUeQxBkQAMAIAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDhtR4gUzU2NlqPgGsoGAwm9fX5ebq+JPvnKVVwBgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACdcDFAqFdM899ygnJ0eDBw/WtGnTdPToUbd3AwBIc64HaPfu3aqoqND+/ftVX1+v8+fP68EHH1RnZ6fbuwIApDHXb8WzY8eObo/Xrl2rwYMHq6mpST/4wQ/c3h0AIE0l/V5wHR0dkqTc3Nwen49Go4pGo/HHkUgk2SMBAFJAUi9CiMViqqys1MSJEzVq1KgetwmFQvL7/fGlqKgomSMBAFJEUgNUUVGhI0eOaNOmTb1uU11drY6OjvgSDoeTORIAIEUk7SO4OXPmaPv27dqzZ49uueWWXrfz+Xzy+XzJGgMAkKJcD5DjOJo7d662bNmixsZGlZSUuL0LAEAGcD1AFRUV2rBhg959913l5OSora1NkuT3+5Wdne327gAAacr13wHV1dWpo6NDwWBQhYWF8WXz5s1u7woAkMaS8hEcAACXw73gAAAmCBAAwAQBAgCYIEAAABMECABgIuk3I0XyBINB6xGQQfh5ujKNjY3WI2QMzoAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw4bUeAKmtsbHRegRkkGT/PAWDwaS+PtzFGRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADCR9AC98sor8ng8qqysTPauAABpJKkBOnTokN544w3dcccdydwNACANJS1AZ86c0cyZM/Xmm2/qpptuStZuAABpKmkBqqio0JQpU1RaWpqsXQAA0lhS7gW3adMmNTc369ChQ5fdNhqNKhqNxh9HIpFkjAQASDGunwGFw2HNnz9f69ev14ABAy67fSgUkt/vjy9FRUVujwQASEGuB6ipqUnt7e2666675PV65fV6tXv3bq1YsUJer1ddXV3dtq+urlZHR0d8CYfDbo8EAEhBrn8E98ADD+ijjz7qtm727NkaPny4Fi5cqKysrG7P+Xw++Xw+t8cAAKQ41wOUk5OjUaNGdVs3cOBA5eXlXbQeAHD94k4IAAAT1+QvovJXNQEA38YZEADABAECAJggQAAAEwQIAGCCAAEATFyTq+CA3gSDQesRABjhDAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATXusBgGRrbGy0HiEtBINB6xH6jO91euEMCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATSQnQZ599pieeeEJ5eXnKzs7W6NGjdfjw4WTsCgCQply/E8IXX3yhiRMnatKkSXrvvff03e9+V8ePH9dNN93k9q4AAGnM9QAtW7ZMRUVFeuutt+LrSkpK3N4NACDNuf4R3LZt2zRu3Dg99thjGjx4sMaOHas333yz1+2j0agikUi3BQCQ+VwP0Mcff6y6ujoNGzZMO3fu1E9/+lPNmzdP69at63H7UCgkv98fX4qKitweCQCQglwPUCwW01133aWlS5dq7NixeuaZZ/T0009r9erVPW5fXV2tjo6O+BIOh90eCQCQglwPUGFhoUaMGNFt3e23365PPvmkx+19Pp8GDRrUbQEAZD7XAzRx4kQdPXq027pjx45pyJAhbu8KAJDGXA/Qc889p/3792vp0qU6ceKENmzYoDVr1qiiosLtXQEA0pjrAbrnnnu0ZcsWbdy4UaNGjdJLL72k2tpazZw50+1dAQDSWFL+JPdDDz2khx56KBkvDQDIENwLDgBgggABAEwQIACACQIEADBBgAAAJpJyFRxwvQkGg0l9/cbGxqS+/rWS7P9OSC+cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE17rAQCkhsbGRusR+iwYDCZ9H9fiv9O1OI5UwBkQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZcD1BXV5dqampUUlKi7Oxs3XrrrXrppZfkOI7buwIApDHX74SwbNky1dXVad26dRo5cqQOHz6s2bNny+/3a968eW7vDgCQplwP0N/+9jc98sgjmjJliiRp6NCh2rhxow4ePOj2rgAAacz1j+Duu+8+NTQ06NixY5KkDz/8UHv37lVZWVmP20ejUUUikW4LACDzuX4GtGjRIkUiEQ0fPlxZWVnq6urSkiVLNHPmzB63D4VC+s1vfuP2GACAFOf6GdDbb7+t9evXa8OGDWpubta6dev0u9/9TuvWretx++rqanV0dMSXcDjs9kgAgBTk+hnQggULtGjRIs2YMUOSNHr0aJ08eVKhUEjl5eUXbe/z+eTz+dweAwCQ4lw/Azp79qz69ev+sllZWYrFYm7vCgCQxlw/A5o6daqWLFmi4uJijRw5Uh988IGWL1+up556yu1dAQDSmOsBWrlypWpqavSzn/1M7e3tCgQC+slPfqLFixe7vSsAQBpzPUA5OTmqra1VbW2t2y8NAMgg3AsOAGCCAAEATBAgAIAJAgQAMEGAAAAmXL8KDoD7gsGg9QiuaGxstB4BKYQzIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEx4rQcAcHmNjY3WIwCu4wwIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYSDhAe/bs0dSpUxUIBOTxeLR169ZuzzuOo8WLF6uwsFDZ2dkqLS3V8ePH3ZoXAJAhEg5QZ2enxowZo1WrVvX4/KuvvqoVK1Zo9erVOnDggAYOHKjJkyfr3LlzfR4WAJA5Er4TQllZmcrKynp8znEc1dbW6le/+pUeeeQRSdKf/vQn5efna+vWrZoxY0bfpgUAZAxXfwfU2tqqtrY2lZaWxtf5/X5NmDBB+/bt6/FrotGoIpFItwUAkPlcDVBbW5skKT8/v9v6/Pz8+HPfFgqF5Pf740tRUZGbIwEAUpT5VXDV1dXq6OiIL+Fw2HokAMA14GqACgoKJEmnT5/utv706dPx577N5/Np0KBB3RYAQOZzNUAlJSUqKChQQ0NDfF0kEtGBAwd07733urkrAECaS/gquDNnzujEiRPxx62trWppaVFubq6Ki4tVWVmpl19+WcOGDVNJSYlqamoUCAQ0bdo0N+cGAKS5hAN0+PBhTZo0Kf64qqpKklReXq61a9fq5z//uTo7O/XMM8/oyy+/1P33368dO3ZowIAB7k0NAEh7CQcoGAzKcZxen/d4PHrxxRf14osv9mkwAEBmM78KDgBwfSJAAAATBAgAYIIAAQBMECAAgImEr4IDkJmCwaD1CH32X0P+K+n7mKu5Sd/H9YIzIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEx4rQfA9a2xsdF6BPyvTPhezNVc6xGQAM6AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiYQDtGfPHk2dOlWBQEAej0dbt26NP3f+/HktXLhQo0eP1sCBAxUIBPTkk0/q1KlTbs4MAMgACQeos7NTY8aM0apVqy567uzZs2publZNTY2am5v1zjvv6OjRo3r44YddGRYAkDkSvhNCWVmZysrKenzO7/ervr6+27rXX39d48eP1yeffKLi4uKrmxIAkHGS/jugjo4OeTwe3XjjjcneFQAgjST1XnDnzp3TwoUL9fjjj2vQoEE9bhONRhWNRuOPI5FIMkcCAKSIpJ0BnT9/XtOnT5fjOKqrq+t1u1AoJL/fH1+KioqSNRIAIIUkJUAX4nPy5EnV19f3evYjSdXV1ero6Igv4XA4GSMBAFKM6x/BXYjP8ePHtWvXLuXl5V1ye5/PJ5/P5/YYAIAUl3CAzpw5oxMnTsQft7a2qqWlRbm5uSosLNSjjz6q5uZmbd++XV1dXWpra5Mk5ebmqn///u5NDgBIawkH6PDhw5o0aVL8cVVVlSSpvLxcv/71r7Vt2zZJ0p133tnt63bt2qVgMHj1kwIAMkrCAQoGg3Icp9fnL/UcAAAXcC84AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABNJvRnp9SxT/p+nTDmOdMf3AZmIMyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYMJrPYCVxsZG6xEA4LrGGRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJhIO0J49ezR16lQFAgF5PB5t3bq1122fffZZeTwe1dbW9mFEAEAmSjhAnZ2dGjNmjFatWnXJ7bZs2aL9+/crEAhc9XAAgMyV8K14ysrKVFZWdsltPvvsM82dO1c7d+7UlClTrno4AEDmcv1ecLFYTLNmzdKCBQs0cuTIy24fjUYVjUbjjyORiNsjAQBSkOsXISxbtkxer1fz5s27ou1DoZD8fn98KSoqcnskAEAKcjVATU1Neu2117R27Vp5PJ4r+prq6mp1dHTEl3A47OZIAIAU5WqA3n//fbW3t6u4uFher1der1cnT57U888/r6FDh/b4NT6fT4MGDeq2AAAyn6u/A5o1a5ZKS0u7rZs8ebJmzZql2bNnu7krAECaSzhAZ86c0YkTJ+KPW1tb1dLSotzcXBUXFysvL6/b9jfccIMKCgp022239X1aAEDGSDhAhw8f1qRJk+KPq6qqJEnl5eVau3ata4MBADJbwgEKBoNyHOeKt//3v/+d6C4AANcB7gUHADBBgAAAJggQAMAEAQIAmHD9XnB9deECh6+//tp4EgDA1bjw7/flLljzOIlc0nYNfPrpp9wPDgAyQDgc1i233NLr8ykXoFgsplOnTiknJ+eK7ycXiURUVFSkcDictrfy4RhSRyYcB8eQGjLhGKTEj8NxHH311VcKBALq16/33/Sk3Edw/fr1u2QxLyUT7iXHMaSOTDgOjiE1ZMIxSIkdh9/vv+w2XIQAADBBgAAAJjIiQD6fTy+88IJ8Pp/1KFeNY0gdmXAcHENqyIRjkJJ3HCl3EQIA4PqQEWdAAID0Q4AAACYIEADABAECAJhI+wCtWrVKQ4cO1YABAzRhwgQdPHjQeqSEhEIh3XPPPcrJydHgwYM1bdo0HT161HqsPnnllVfk8XhUWVlpPUpCPvvsMz3xxBPKy8tTdna2Ro8ercOHD1uPdcW6urpUU1OjkpISZWdn69Zbb9VLL72U0B+QtLBnzx5NnTpVgUBAHo9HW7du7fa84zhavHixCgsLlZ2drdLSUh0/ftxm2F5c6hjOnz+vhQsXavTo0Ro4cKACgYCefPJJnTp1ym7gHlzu+/D/Pfvss/J4PKqtre3TPtM6QJs3b1ZVVZVeeOEFNTc3a8yYMZo8ebLa29utR7tiu3fvVkVFhfbv36/6+nqdP39eDz74oDo7O61HuyqHDh3SG2+8oTvuuMN6lIR88cUXmjhxom644Qa99957+sc//qHf//73uummm6xHu2LLli1TXV2dXn/9df3zn//UsmXL9Oqrr2rlypXWo11SZ2enxowZo1WrVvX4/KuvvqoVK1Zo9erVOnDggAYOHKjJkyfr3Llz13jS3l3qGM6ePavm5mbV1NSoublZ77zzjo4ePaqHH37YYNLeXe77cMGWLVu0f/9+BQKBvu/USWPjx493Kioq4o+7urqcQCDghEIhw6n6pr293ZHk7N6923qUhH311VfOsGHDnPr6eueHP/yhM3/+fOuRrtjChQud+++/33qMPpkyZYrz1FNPdVv3ox/9yJk5c6bRRImT5GzZsiX+OBaLOQUFBc5vf/vb+Lovv/zS8fl8zsaNGw0mvLxvH0NPDh486EhyTp48eW2GSlBvx/Dpp5863/ve95wjR444Q4YMcf7whz/0aT9pewb0zTffqKmpSaWlpfF1/fr1U2lpqfbt22c4Wd90dHRIknJzc40nSVxFRYWmTJnS7XuSLrZt26Zx48bpscce0+DBgzV27Fi9+eab1mMl5L777lNDQ4OOHTsmSfrwww+1d+9elZWVGU929VpbW9XW1tbtZ8rv92vChAlp/z73eDy68cYbrUe5YrFYTLNmzdKCBQs0cuRIV14z5W5GeqU+//xzdXV1KT8/v9v6/Px8/etf/zKaqm9isZgqKys1ceJEjRo1ynqchGzatEnNzc06dOiQ9ShX5eOPP1ZdXZ2qqqr0i1/8QocOHdK8efPUv39/lZeXW493RRYtWqRIJKLhw4crKytLXV1dWrJkiWbOnGk92lVra2uTpB7f5xeeSzfnzp3TwoUL9fjjj6fVDUqXLVsmr9erefPmufaaaRugTFRRUaEjR45o79691qMkJBwOa/78+aqvr9eAAQOsx7kqsVhM48aN09KlSyVJY8eO1ZEjR7R69eq0CdDbb7+t9evXa8OGDRo5cqRaWlpUWVmpQCCQNseQ6c6fP6/p06fLcRzV1dVZj3PFmpqa9Nprr6m5ufmK/0zOlUjbj+BuvvlmZWVl6fTp093Wnz59WgUFBUZTXb05c+Zo+/bt2rVr11X/OQorTU1Nam9v11133SWv1yuv16vdu3drxYoV8nq96urqsh7xsgoLCzVixIhu626//XZ98sknRhMlbsGCBVq0aJFmzJih0aNHa9asWXruuecUCoWsR7tqF97LmfA+vxCfkydPqr6+Pq3Oft5//321t7eruLg4/h4/efKknn/+eQ0dOvSqXzdtA9S/f3/dfffdamhoiK+LxWJqaGjQvffeazhZYhzH0Zw5c7Rlyxb99a9/VUlJifVICXvggQf00UcfqaWlJb6MGzdOM2fOVEtLi7KysqxHvKyJEydedPn7sWPHNGTIEKOJEnf27NmL/vhXVlaWYrGY0UR9V1JSooKCgm7v80gkogMHDqTV+/xCfI4fP66//OUvysvLsx4pIbNmzdLf//73bu/xQCCgBQsWaOfOnVf9umn9EVxVVZXKy8s1btw4jR8/XrW1ters7NTs2bOtR7tiFRUV2rBhg959913l5OTEP9f2+/3Kzs42nu7K5OTkXPQ7q4EDByovLy9tfpf13HPP6b777tPSpUs1ffp0HTx4UGvWrNGaNWusR7tiU6dO1ZIlS1RcXKyRI0fqgw8+0PLly/XUU09Zj3ZJZ86c0YkTJ+KPW1tb1dLSotzcXBUXF6uyslIvv/yyhg0bppKSEtXU1CgQCGjatGl2Q3/LpY6hsLBQjz76qJqbm7V9+3Z1dXXF3+e5ubnq37+/1djdXO778O1o3nDDDSooKNBtt9129Tvt0zV0KWDlypVOcXGx079/f2f8+PHO/v37rUdKiKQel7feest6tD5Jt8uwHcdx/vznPzujRo1yfD6fM3z4cGfNmjXWIyUkEok48+fPd4qLi50BAwY43//+951f/vKXTjQatR7tknbt2tXje6C8vNxxnP+5FLumpsbJz893fD6f88ADDzhHjx61HfpbLnUMra2tvb7Pd+3aZT163OW+D9/mxmXY/DkGAICJtP0dEAAgvREgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJv4b5ta5SPq3DFkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The helper functions\n",
    "\n",
    "from IPython import get_ipython\n",
    "import random\n",
    "from mazelab.generators import random_maze, morris_water_maze\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from mazelab.solvers import dijkstra_solver\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from environment import TaskEnv\n",
    "from typing import Tuple, List\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def animate_run(data:List[np.ndarray]):\n",
    "    init_img = data[0]\n",
    "    remaining_img = data[1:]\n",
    "    img_container = plt.imshow(init_img)  # only call this once\n",
    "    for img in remaining_img:\n",
    "        img_container.set_data(img)  # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def visualize_agent_brain(agent, env: TaskEnv):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax1.set_title(\"Highest state value at position (x,y)\")\n",
    "    state_value_map = agent.q_table.max(axis=2)\n",
    "    sns.heatmap(state_value_map, ax=ax1)\n",
    "\n",
    "    ax2.set_title(\"Chosen action at position (x,y)\")\n",
    "    n = env.action_space.n + 1\n",
    "    path = env.maze.objects.free.positions\n",
    "    decisions_map = np.array([[x_, y_, agent.select_action([x_, y_]) + 1] for x_, y_ in path])\n",
    "    state_action_map = np.zeros_like(agent.q_table.max(axis=2))\n",
    "    state_action_map[decisions_map[:, 0], decisions_map[:, 1]] = decisions_map[:, 2]\n",
    "    cmap = sns.color_palette(\"viridis\", n)\n",
    "    sns.heatmap(state_action_map, cmap=cmap, ax=ax2)\n",
    "    colorbar = ax2.collections[0].colorbar\n",
    "    r = (colorbar.vmax) - colorbar.vmin\n",
    "    colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "    colorbar.set_ticklabels(['N/A', 'north', 'south', 'west', 'east'])\n",
    "    fig.tight_layout()\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "env = TaskEnv()\n",
    "env.reset()\n",
    "impassable_array = env.unwrapped.maze.to_impassable()\n",
    "motions = env.unwrapped.motions\n",
    "start = env.unwrapped.maze.objects.agent.positions[0]\n",
    "goal = env.unwrapped.maze.objects.goal.positions[0]\n",
    "actions = dijkstra_solver(impassable_array, motions, start, goal)\n",
    "print(actions)\n",
    "\n",
    "imgs = []\n",
    "rewards = 0.0\n",
    "for action in actions:\n",
    "    _, reward, _, _ = env.step(action)\n",
    "    rewards += reward\n",
    "    imgs.append(env.render(\"rgb_array\"))\n",
    "print(rewards)\n",
    "\n",
    "animate_run(imgs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Implement the agents \n",
    "\n",
    "In this part, you are expected to implement two RL agents. \n",
    "\n",
    "- Agent 1 uses the Q-learning algorithm to learn the optimal solution\n",
    "- Agent 2 uses the SARSA algorithm to learn the optimal solution. To decide the action to take at each time step,  this agent uses the epsilon greedy action selection.\n",
    "\n",
    "Here we also provided an example agent: Random Agent. It follows a random policy to move at each step (randomly select the action). You can use this example agent as a baseline to evaluate your agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "class RandomAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = None,\n",
    "                 learning_rate: float = None,\n",
    "                 discount_factor: float = None) -> int:\n",
    "        self.epsilon = 1  # A random agent \"explores\" always, so epsilon will be 1\n",
    "        self.alpha = 0  # A random agent never learns, so there's no need for a learning rate\n",
    "        self.gamma = 0  # A random agent does not update it's q-table. Hence, it's zero.\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = 0.1,\n",
    "                 learning_rate: float = 0.8,\n",
    "                 discount_factor: float = 0.95) -> int:\n",
    "        self.epsilon = exploration_rate\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        x, y = state\n",
    "        x_, y_ = next_state\n",
    "        max_val = np.max(self.q_table[x_, y_, :])\n",
    "        self.q_table[x, y, action] = (1 - self.alpha) * self.q_table[x, y, action] + self.alpha * (reward + self.gamma * max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = 0.6,\n",
    "                 learning_rate: float = 0.5,\n",
    "                 discount_factor: float = 0) -> int:\n",
    "        self.epsilon = exploration_rate  # A random agent \"explores\" always, so epsilon will be 1\n",
    "        self.alpha = learning_rate  # A random agent never learns, so there's no need for a learning rate\n",
    "        self.gamma = discount_factor  # A random agent does not update it's q-table. Hence, it's zero.\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        action = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state, :])\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, state2, reward, action, action2): \n",
    "        predict = self.q_table[state, action] \n",
    "        target = reward + self.gamma * self.q_table[state2, action2] \n",
    "        self.q_table[state, action] = self.q_table[state, action] + self.alpha * (target - predict)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run the simulation\n",
    "\n",
    "Now, we write the codes for running a simulation. In each run, you shall setup the epsilon parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     imgs\u001b[39m.\u001b[39mappend(env\u001b[39m.\u001b[39mrender(\u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(rewards)\n\u001b[0;32m---> 22\u001b[0m animate_run(imgs)\n\u001b[1;32m     23\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36manimate_run\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m remaining_img:\n\u001b[1;32m     23\u001b[0m     img_container\u001b[39m.\u001b[39mset_data(img)  \u001b[39m# just update the data\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     display\u001b[39m.\u001b[39;49mdisplay(plt\u001b[39m.\u001b[39;49mgcf())\n\u001b[1;32m     25\u001b[0m     display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    175\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    222\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    339\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    340\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/backend_bases.py:2314\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     renderer \u001b[39m=\u001b[39m _get_renderer(\n\u001b[1;32m   2309\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure,\n\u001b[1;32m   2310\u001b[0m         functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m   2311\u001b[0m             print_method, orientation\u001b[39m=\u001b[39morientation)\n\u001b[1;32m   2312\u001b[0m     )\n\u001b[1;32m   2313\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mgetattr\u001b[39m(renderer, \u001b[39m\"\u001b[39m\u001b[39m_draw_disabled\u001b[39m\u001b[39m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2314\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m   2316\u001b[0m \u001b[39mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2317\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/figure.py:3082\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3079\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3081\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3082\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3083\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3085\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3086\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axes/_base.py:3100\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3097\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3098\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3100\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3101\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3103\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axis.py:1314\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     tick\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1310\u001b[0m \u001b[39m# Scale up the axis label box to also find the neighbors, not just the\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39m# tick labels that actually overlap.  We need a *copy* of the axis\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[39m# label box because we don't want to scale the actual bbox.\u001b[39;00m\n\u001b[0;32m-> 1314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_label_position(renderer)\n\u001b[1;32m   1316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1318\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_offset_text_position(tlb1, tlb2)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axis.py:2252\u001b[0m, in \u001b[0;36mXAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2248\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   2250\u001b[0m \u001b[39m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[1;32m   2251\u001b[0m \u001b[39m# that have been set by `fig.align_xlabels()`\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m bboxes, bboxes2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_tick_boxes_siblings(renderer\u001b[39m=\u001b[39;49mrenderer)\n\u001b[1;32m   2254\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mget_position()\n\u001b[1;32m   2255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_position \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottom\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axis.py:2055\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m grouper\u001b[39m.\u001b[39mget_siblings(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes):\n\u001b[1;32m   2054\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(ax, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39maxis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2055\u001b[0m     ticks_to_draw \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49m_update_ticks()\n\u001b[1;32m   2056\u001b[0m     tlb, tlb2 \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[1;32m   2057\u001b[0m     bboxes\u001b[39m.\u001b[39mextend(tlb)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axis.py:1190\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_ticks\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1186\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m    Update ticks (position and labels) using the current data interval of\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39m    the axes.  Return the list of ticks that will be drawn.\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m     major_locs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_majorticklocs()\n\u001b[1;32m   1191\u001b[0m     major_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmajor\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39mformat_ticks(major_locs)\n\u001b[1;32m   1192\u001b[0m     major_ticks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_major_ticks(\u001b[39mlen\u001b[39m(major_locs))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axis.py:1416\u001b[0m, in \u001b[0;36mAxis.get_majorticklocs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_majorticklocs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1415\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return this Axis' major tick locations in data coordinates.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1416\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmajor\u001b[39m.\u001b[39;49mlocator()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/ticker.py:2142\u001b[0m, in \u001b[0;36mMaxNLocator.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2142\u001b[0m     vmin, vmax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis\u001b[39m.\u001b[39;49mget_view_interval()\n\u001b[1;32m   2143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtick_values(vmin, vmax)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axis.py:2162\u001b[0m, in \u001b[0;36m_make_getset_interval.<locals>.getter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetter\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2161\u001b[0m     \u001b[39m# docstring inherited.\u001b[39;00m\n\u001b[0;32m-> 2162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxes, lim_name), attr_name)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axes/_base.py:815\u001b[0m, in \u001b[0;36m_AxesBase.viewLim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    814\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mviewLim\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unstale_viewLim()\n\u001b[1;32m    816\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viewLim\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axes/_base.py:802\u001b[0m, in \u001b[0;36m_AxesBase._unstale_viewLim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unstale_viewLim\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    800\u001b[0m     \u001b[39m# We should arrange to store this information once per share-group\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[39m# instead of on every axis.\u001b[39;00m\n\u001b[0;32m--> 802\u001b[0m     need_scale \u001b[39m=\u001b[39m {\n\u001b[1;32m    803\u001b[0m         name: \u001b[39many\u001b[39m(ax\u001b[39m.\u001b[39m_stale_viewlims[name]\n\u001b[1;32m    804\u001b[0m                   \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shared_axes[name]\u001b[39m.\u001b[39mget_siblings(\u001b[39mself\u001b[39m))\n\u001b[1;32m    805\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axis_names}\n\u001b[1;32m    806\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(need_scale\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    807\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m need_scale:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axes/_base.py:803\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unstale_viewLim\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    800\u001b[0m     \u001b[39m# We should arrange to store this information once per share-group\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[39m# instead of on every axis.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     need_scale \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 803\u001b[0m         name: \u001b[39many\u001b[39;49m(ax\u001b[39m.\u001b[39;49m_stale_viewlims[name]\n\u001b[1;32m    804\u001b[0m                   \u001b[39mfor\u001b[39;49;00m ax \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shared_axes[name]\u001b[39m.\u001b[39;49mget_siblings(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m    805\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axis_names}\n\u001b[1;32m    806\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(need_scale\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    807\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m need_scale:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/axes/_base.py:803\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unstale_viewLim\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    800\u001b[0m     \u001b[39m# We should arrange to store this information once per share-group\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[39m# instead of on every axis.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     need_scale \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 803\u001b[0m         name: \u001b[39many\u001b[39m(ax\u001b[39m.\u001b[39m_stale_viewlims[name]\n\u001b[1;32m    804\u001b[0m                   \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shared_axes[name]\u001b[39m.\u001b[39mget_siblings(\u001b[39mself\u001b[39m))\n\u001b[1;32m    805\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axis_names}\n\u001b[1;32m    806\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(need_scale\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    807\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m need_scale:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = QLearningAgent(env, exploration_rate=1, learning_rate=0.5, discount_factor=0.3)\n",
    "\n",
    "imgs = []\n",
    "rewards = 0.0\n",
    "state = env.reset()\n",
    "\n",
    "goal_pos = env.unwrapped.maze.objects.goal.positions[0]\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.select_action(state)\n",
    "    next_state, reward, _, _ = env.step(action)\n",
    "    agent.learn(state, action, next_state, reward, done)\n",
    "    if tuple(next_state) == tuple(goal_pos):\n",
    "        done = True\n",
    "    state = next_state\n",
    "    rewards += reward\n",
    "    imgs.append(env.render(\"rgb_array\"))\n",
    "\n",
    "print(rewards)\n",
    "\n",
    "animate_run(imgs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode:  -3411.259399999999\n",
      "Success rate:  1.0\n"
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent(env, exploration_rate=1, learning_rate=0.5, discount_factor=0.3)\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "total_rewards = 0.0\n",
    "success_episodes = 0\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    rewards = 0.0\n",
    "    state = env.reset()\n",
    "    goal_pos = env.unwrapped.maze.objects.goal.positions[0]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, _, _ = env.step(action)\n",
    "        agent.learn(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "        rewards += reward\n",
    "\n",
    "        if tuple(next_state) == tuple(goal_pos):\n",
    "            done = True\n",
    "            success_episodes += 1\n",
    "    \n",
    "    total_rewards += rewards\n",
    "    \n",
    "average_reward = total_rewards / num_episodes\n",
    "success_rate = success_episodes / num_episodes\n",
    "\n",
    "print(\"Average reward per episode: \", average_reward)\n",
    "print(\"Success rate: \", success_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Play with parameters and analyse results\n",
    " \n",
    "Finally, you will describe, evaluate and interpret your results from two RL agents, as well as compare your agents with the given Random agent. Feel free to use the provided helper functions for evaluating your agents. Some points are good to notice:\n",
    "\n",
    "- Both quantified evaluation and human evaluation are needed in the report. The quantified evaluation shall focus on the measurement of reward. In human evaluation, you can use the provided visual tools to interpret your results. Your report shall include at least one plot presenting comparable measures of the different agents. \n",
    "\n",
    "- While evaluating the results of Agent 2 (with SARSA algorithm), please try at least 2 different values of **epsilon** (expect 0) and discuss the influence of different epsilon values on results. In the end, please identify a reasonable epsilon value that could balance the exploration and exploitation, then fix this value for comparing two agents. Present your trails and results in the report.\n",
    "\n",
    "- In the report, you also need to parcitularly describe and discuss the similarity and difference of results from two RL agents (hint: on-policy VS off-policy). For this, please make sure that the compared results are obtained from the same environment (a same maze for two different agents). Also, while evaluating the results of two agents, please try at least 2 different values of **gamma**. In this way, you could discuss the influence of this discount factor in your report. \n",
    "\n",
    "- Please run the simulation for multiple times and average them for all your results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus task. For each task that is successfully completed, you may obtain max. 1 extra point. \n",
    "\n",
    "1. Implement a third RL agent using another RL algorithm (e.g. Monte Carlo methods, Expected SARSA or even neural network-based ones) and discuss your findings. Compare this third agent with the above ones and explain why this is a better (or worse) RL algorithm. You are allowed to reuse exsiting packages, but please cite them, test them in advance, and make sure that you can explain the used algorithm using your own words.\n",
    "\n",
    "2. Can you explore and show other evaluation results? If so, implement and present one extra result (e.g. a plot). And please explain why it is a good evaluation for our task or how it shows the difference between two RL agents/algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
