{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 – Finding the way out in a maze\n",
    "\n",
    "*Due: Friday January 27 at 17:00 CET*\n",
    "\n",
    "In the forth assignment of the course Applications of Machine Learning (INFOB3APML), you will learn to use RL algorithms to solve the practical problem of ‘robot in a maze’. The objectives of this assignment are:\n",
    "- learn to formalize a practical problem into the Markov Decision Process (MDP)\n",
    "- get familier with the OpenAI gym framework (recently renamed as Gymnasium) and use it to implement RL agents\n",
    "- use the SARSA and Q-learning algorithm to solve the ‘robot in a maze’ MDP problem\n",
    "- evaluate the results of reinforcement learning and interpret your findings\n",
    "- reflect on the difference between two type of RL algorithms\n",
    "\n",
    "In this assignment, you are going to develop a robot and find its way out in a maze. The project is divided into three parts (5 subtask):\n",
    "-\tIn the first part (1), you will get familier with the OpenAI gym/gymnasium framework. \n",
    "-\tIn the second part, based on the gym/gymnasium framework, we have implemented the environment for you (2.1). Your task is to formalize the problem as a MDP model (2.0), implement your own RL agents (2.2) and to train them for finding the shortest route to get out of a maze (2.3).\n",
    "-\tIn the third part (3), you will evaluate and interpret your results from the implemented RL agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's start with the OpenAI gym\n",
    "\n",
    "Gym/Gynasium (https://gymnasium.farama.org/) is a wide-used standard toolkit for developing and comparing reinforcement learning algorithms. Gynasium is the maintained fork of OpenAI’s Gym library (more story about this recent change if you are interested: https://farama.org/Announcing-The-Farama-Foundation).\n",
    "\n",
    "1. Gym/Gynasium makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. \n",
    "\n",
    "2. The library is a collection of test problems — **environments** — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.\n",
    "\n",
    "First, we download & install the gym/gynasium library. Then import the gymnasium class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "scipy 1.7.1 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.24.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached gymnasium-0.27.0-py3-none-any.whl (879 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\tim19\\anaconda3\\lib\\site-packages (from gymnasium) (4.8.1)\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.24.1-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting shimmy<1.0,>=0.1.0\n",
      "  Using cached Shimmy-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting jax-jumpy>=0.2.0\n",
      "  Using cached jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\n",
      "Collecting gymnasium-notices>=0.0.1\n",
      "  Using cached gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
      "Collecting typing-extensions>=4.3.0\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tim19\\anaconda3\\lib\\site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tim19\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.6.0)\n",
      "Installing collected packages: numpy, typing-extensions, shimmy, jax-jumpy, gymnasium-notices, gymnasium\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "Successfully installed gymnasium-0.27.0 gymnasium-notices-0.0.1 jax-jumpy-0.2.0 numpy-1.24.1 shimmy-0.2.0 typing-extensions-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to explain how the RL framework of gym works. \n",
    "- An **ENVIRONMENT**, \n",
    "- You also have an **AGENT**,\n",
    "- In MDP problems (like ours), the **ENVIRONMENT** will also provides an **OBSERVATION**, which represets the state of the **ENVIRONMENT** at the current moment.\n",
    "- The agent takes an **ACTION** based on its **OBSERVATION**,\n",
    "- When a single **ACTION** is chosen and fed to our **ENVIRONMENT**, the **ENVIRONMENT** measures how good the action was taken and produces a **REWARD**, which is usually a numeric value.\n",
    "\n",
    "Please read the 'Basic usage' https://gymnasium.farama.org/content/basic_usage/ for better understanding the framework.  And do not forget import gymnasium before running other codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Go back to our own task\n",
    " \n",
    " Next, you will solve a practical MDP problem 'robot in a maze' based on the gym framework. You shall implement the RL agent and train it to find the shortest route to achieve the maze goal. In this MDP, the enviroment is a grid world (a maze) while the agent is a robot. At each time step, the robot starts at a random location and can move around in the grid world. The long-term objective is finding the way out (reaching the final location). Hence, you need to find a fixed goal position within the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Model the practical task into a MDP\n",
    "\n",
    "To solve a RL problem, we start with formalizing the problem into a MDP model. Please describe this MDP model in your report. \n",
    "\n",
    "Notice: No empricial data provided in this assignment, so the point of 'data description and exploration' will be given to this step. \n",
    "\n",
    "While exploring your MDP model, you shall think about questions such as:\n",
    "- What is the environment? How does it look like?\n",
    "- What simulated data can your RL agent observe from the environment? How does it look like?\n",
    "- Which data is considered as the state? Which data is considered as the reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set up the environment\n",
    "\n",
    "There is no need to implement your own environment. You shall use the environment we provide in the file **environment.py**. But please make sure to have a look at it, so that you understand the inner working of this environment.\n",
    "\n",
    "The core gym interface is **Env**, which is the unified environment interface. The following are the Env methods you should know:\n",
    "\n",
    "- reset(self): Reset the environment's state. Returns observation.\n",
    "- step(self, action): Step the environment by one timestep. Returns observation, reward, done, info. \n",
    "- render(self, mode='rgb_array'): Render one frame of the environment. The default mode will do something human friendly, such as pop up a window. In this assignment, there is no need to create a pop up window. \n",
    "\n",
    "Please notice that you need to first install the [mazelab](https://github.com/zuoxingdong/mazelab) package for running the environment (a file with required packages is also given). If you run the below cell the first time. Make sure to restart the ipython notebook at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sw1989/mazelab.git\n",
    "!pip install -e mazelab\n",
    "!pip install pandas\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check whether the required packages (e.g. mazela, pandas, tqdm, seaborn) are installed. Please install the ones are missing. \n",
    "\n",
    "ATTENTION: To run the given code, please use the python version 3.7-3.9, and the numpy version < 1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a few helper functions to make it easier to debug your agents. \n",
    " - `animate_run` will enable you to see the agent's behavior. It takes a list of images which can be produced by the `env.render` function of the environment\n",
    " - `visualize_agent_brain` will provide you with a way to visualize the agents learned q_table. Use it after you have implemented and trained your agents. The first plot will show the highest q-value per state (position on the map) and the second will tell you which action the agent would choose at that state/position. It takes the environment and the agent as input.\n",
    "\n",
    "Below you will find a basic example of how the animation function works. Please notice that: whenever you **reset()** the environment, the agent will start at a random position (a different state). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The helper functions\n",
    "\n",
    "from IPython import get_ipython\n",
    "import random\n",
    "from mazelab.generators import random_maze, morris_water_maze\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from mazelab.solvers import dijkstra_solver\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from environment import TaskEnv\n",
    "from typing import Tuple, List\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def animate_run(data:List[np.ndarray]):\n",
    "    init_img = data[0]\n",
    "    remaining_img = data[1:]\n",
    "    img_container = plt.imshow(init_img)  # only call this once\n",
    "    for img in remaining_img:\n",
    "        img_container.set_data(img)  # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def visualize_agent_brain(agent, env: TaskEnv):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax1.set_title(\"Highest state value at position (x,y)\")\n",
    "    state_value_map = agent.q_table.max(axis=2)\n",
    "    sns.heatmap(state_value_map, ax=ax1)\n",
    "\n",
    "    ax2.set_title(\"Chosen action at position (x,y)\")\n",
    "    n = env.action_space.n + 1\n",
    "    path = env.maze.objects.free.positions\n",
    "    decisions_map = np.array([[x_, y_, agent.select_action([x_, y_]) + 1] for x_, y_ in path])\n",
    "    state_action_map = np.zeros_like(agent.q_table.max(axis=2))\n",
    "    state_action_map[decisions_map[:, 0], decisions_map[:, 1]] = decisions_map[:, 2]\n",
    "    cmap = sns.color_palette(\"viridis\", n)\n",
    "    sns.heatmap(state_action_map, cmap=cmap, ax=ax2)\n",
    "    colorbar = ax2.collections[0].colorbar\n",
    "    r = (colorbar.vmax) - colorbar.vmin\n",
    "    colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "    colorbar.set_ticklabels(['N/A', 'north', 'south', 'west', 'east'])\n",
    "    fig.tight_layout()\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "env = TaskEnv()\n",
    "env.reset()\n",
    "impassable_array = env.unwrapped.maze.to_impassable()\n",
    "motions = env.unwrapped.motions\n",
    "start = env.unwrapped.maze.objects.agent.positions[0]\n",
    "goal = env.unwrapped.maze.objects.goal.positions[0]\n",
    "actions = dijkstra_solver(impassable_array, motions, start, goal)\n",
    "print(actions)\n",
    "\n",
    "imgs = []\n",
    "rewards = 0.0\n",
    "for action in actions:\n",
    "    _, reward, _, _ = env.step(action)\n",
    "    rewards += reward\n",
    "    imgs.append(env.render(\"rgb_array\"))\n",
    "print(rewards)\n",
    "\n",
    "animate_run(imgs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Implement the agents \n",
    "\n",
    "In this part, you are expected to implement two RL agents. \n",
    "\n",
    "- Agent 1 uses the Q-learning algorithm to learn the optimal solution\n",
    "- Agent 2 uses the SARSA algorithm to learn the optimal solution. To decide the action to take at each time step,  this agent uses the epsilon greedy action selection.\n",
    "\n",
    "Here we also provided an example agent: Random Agent. It follows a random policy to move at each step (randomly select the action). You can use this example agent as a baseline to evaluate your agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "class RandomAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = None,\n",
    "                 learning_rate: float = None,\n",
    "                 discount_factor: float = None) -> int:\n",
    "        self.epsilon = 1  # A random agent \"explores\" always, so epsilon will be 1\n",
    "        self.alpha = 0  # A random agent never learns, so there's no need for a learning rate\n",
    "        self.gamma = 0  # A random agent does not update it's q-table. Hence, it's zero.\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = 0.1,\n",
    "                 learning_rate: float = 0.8,\n",
    "                 discount_factor: float = 0.95) -> int:\n",
    "        self.epsilon = exploration_rate\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        x, y = state\n",
    "        x_, y_ = next_state\n",
    "        max_val = np.max(self.q_table[x_, y_, :])\n",
    "        self.q_table[x, y, action] = (1 - self.alpha) * self.q_table[x, y, action] + self.alpha * (reward + self.gamma * max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q learning agent version 2\n",
    "class QLearningAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = 0.1,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 discount_factor: float = 0.9) -> int:\n",
    "        self.epsilon = exploration_rate\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        x,y = state\n",
    "        x_,y_ = next_state\n",
    "        max_next_q = max(self.q_table[x_,y_,:])\n",
    "        self.q_table[x,y,action] = self.q_table[x,y,action] + self.alpha * (reward + self.gamma * max_next_q - self.q_table[x,y,action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run the simulation\n",
    "\n",
    "Now, we write the codes for running a simulation. In each run, you shall setup the epsilon parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Play with parameters and analyse results\n",
    " \n",
    "Finally, you will describe, evaluate and interpret your results from two RL agents, as well as compare your agents with the given Random agent. Feel free to use the provided helper functions for evaluating your agents. Some points are good to notice:\n",
    "\n",
    "- Both quantified evaluation and human evaluation are needed in the report. The quantified evaluation shall focus on the measurement of reward. In human evaluation, you can use the provided visual tools to interpret your results. Your report shall include at least one plot presenting comparable measures of the different agents. \n",
    "\n",
    "- While evaluating the results of Agent 2 (with SARSA algorithm), please try at least 2 different values of **epsilon** (expect 0) and discuss the influence of different epsilon values on results. In the end, please identify a reasonable epsilon value that could balance the exploration and exploitation, then fix this value for comparing two agents. Present your trails and results in the report.\n",
    "\n",
    "- In the report, you also need to parcitularly describe and discuss the similarity and difference of results from two RL agents (hint: on-policy VS off-policy). For this, please make sure that the compared results are obtained from the same environment (a same maze for two different agents). Also, while evaluating the results of two agents, please try at least 2 different values of **gamma**. In this way, you could discuss the influence of this discount factor in your report. \n",
    "\n",
    "- Please run the simulation for multiple times and average them for all your results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus task. For each task that is successfully completed, you may obtain max. 1 extra point. \n",
    "\n",
    "1. Implement a third RL agent using another RL algorithm (e.g. Monte Carlo methods, Expected SARSA or even neural network-based ones) and discuss your findings. Compare this third agent with the above ones and explain why this is a better (or worse) RL algorithm. You are allowed to reuse exsiting packages, but please cite them, test them in advance, and make sure that you can explain the used algorithm using your own words.\n",
    "\n",
    "2. Can you explore and show other evaluation results? If so, implement and present one extra result (e.g. a plot). And please explain why it is a good evaluation for our task or how it shows the difference between two RL agents/algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d23ce8f6a59134e79a0198c12c88cfa5f56d45eb2f0aec4416bb561bd372b36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
